(window.webpackJsonp=window.webpackJsonp||[]).push([[35],{430:function(t,a,e){"use strict";e.r(a);var r=e(45),s=Object(r.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"应用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#应用"}},[t._v("#")]),t._v(" 应用")]),t._v(" "),e("h2",{attrs:{id:"爬虫"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#爬虫"}},[t._v("#")]),t._v(" 爬虫")]),t._v(" "),e("h3",{attrs:{id:"思路"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#思路"}},[t._v("#")]),t._v(" 思路")]),t._v(" "),e("p",[t._v("比如说我们要爬取一个网页的数据，那么大致是以下流程")]),t._v(" "),e("ul",[e("li",[t._v("发送请求")]),t._v(" "),e("li",[t._v("获取数据")]),t._v(" "),e("li",[t._v("解析数据")])]),t._v(" "),e("p",[t._v("有些网站有反爬虫，那么就需要对应的反爬虫策略，比如伪造"),e("code",[t._v("header")]),t._v("、代理"),e("code",[t._v("ip")]),t._v("等")]),t._v(" "),e("h2",{attrs:{id:"web服务器"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#web服务器"}},[t._v("#")]),t._v(" web服务器")]),t._v(" "),e("h2",{attrs:{id:"中间件"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#中间件"}},[t._v("#")]),t._v(" 中间件")])])}),[],!1,null,null,null);a.default=s.exports}}]);